<!DOCTYPE HTML>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Video Capture Example</title>
<link href="./Video Capture Example_files/js_example_style.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"> </script>
</head>

<body>
<h2>Video Capture Example</h2>
<p>
    Click <b>Start/Stop</b> button to start or stop the camera capture.<br>
    The <b>videoInput</b> is a &lt;video&gt; element used as OpenCV.js input.
    The <b>canvasOutput</b> is a &lt;canvas&gt; element used as OpenCv.js output.<br>
    The code of &lt;textarea&gt; will be executed when video is started.
    You can modify the code to investigate more.
</p>
<div>
<div class="control"><button id="startAndStop">Start</button></div>
<textarea class="code" rows="29" cols="100" id="codeEditor" spellcheck="false"></textarea>
</div>
<p class="err" id="errorMessage"></p>
<div>
    <table cellpadding="0" cellspacing="0" width="0" border="0">
    <tbody><tr>
        <td>
            <video id="videoInput" style="display:none" width="320" height="240"></video>
        </td>
        <td>
            <canvas id="canvasOutput" width="320" height="240"></canvas>
        </td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>
            <div class="caption" style="display:none">videoInput</div>
        </td>
        <td>
            <div class="caption">canvasOutput</div>
        </td>
        <td></td>
        <td></td>
    </tr>
    </tbody></table>
</div>
<!-- <script async src="https://grafias-dev.s3.eu-central-1.amazonaws.com/js/opencv.js" type="text/javascript"></script> -->

<script src="./Video Capture Example_files/adapter-5.0.4.js" type="text/javascript"></script>
<script src="./Video Capture Example_files/utils.js" type="text/javascript"></script>
<script id="codeSnippet" type="text/code-snippet">
    const FPS = 30;
    function processVideo() {
        try {
            let video = document.getElementById('videoInput');
            let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
            let cap = new cv.VideoCapture(video);
            if (!streaming) {
                // clean and stop.
                src.delete();
                return;
            }

            let begin = Date.now();
            // start processing.
            cap.read(src);
    
            let gray = new cv.Mat();
            cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0);
            let faces = new cv.RectVector();
            let eyes = new cv.RectVector();
            let faceCascade = new cv.CascadeClassifier();
            let eyeCascade = new cv.CascadeClassifier();
            // load pre-trained classifiers
            faceCascade.load('haarcascade_frontalface_default.xml');
            eyeCascade.load('haarcascade_eye.xml');
            // detect faces
             // detect faces
            let msize = new cv.Size(0, 0);
            faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0, msize, msize);
            for (let i = 0; i < faces.size(); ++i) {
                let roiGray = gray.roi(faces.get(i));
                let roiSrc = src.roi(faces.get(i));
                let point1 = new cv.Point(faces.get(i).x, faces.get(i).y);
                let point2 = new cv.Point(faces.get(i).x + faces.get(i).width,
                                        faces.get(i).y + faces.get(i).height);
                cv.rectangle(src, point1, point2, [255, 0, 0, 255]);
                testData.push(faces.get(i).x - 0, faces.get(i).y - 0)
                console.log("iterasi ke:", testData.length)
                if (testData.length === 900) {
                    const output = model.predict(tf.tensor([[testData]]));
                    const outputData = output.dataSync();
                    console.log(outputData)
                    testData = []
                }

                // detect eyes in face ROI
                eyeCascade.detectMultiScale(roiGray, eyes);
                for (let j = 0; j < eyes.size(); ++j) {
                    let point1 = new cv.Point(eyes.get(j).x, eyes.get(j).y);
                    let point2 = new cv.Point(eyes.get(j).x + eyes.get(j).width,
                                            eyes.get(j).y + eyes.get(j).height);
                    cv.rectangle(roiSrc, point1, point2, [0, 0, 255, 255]);
                }
                roiGray.delete(); roiSrc.delete();
            }
            cv.imshow('canvasOutput', src);
            src.delete(); 
            gray.delete(); 
            faceCascade.delete();
            eyeCascade.delete(); 
            faces.delete(); 
            eyes.delete();

            // schedule the next one.
            let delay = 1000/FPS - (Date.now() - begin);
            console.log(delay)
            setTimeout(processVideo, delay);
        } catch (err) {
            utils.printError(err);
        }
    };
    
    // schedule the first one.
    setTimeout(processVideo, 0);
    
</script>
<script type="text/javascript">
    let model = null;
    let testData = []
    async function loadModel() {
        model = await tf.loadLayersModel("http://localhost/video/model.json");
        tf.tensor([1, 2, 3, 4]).print()
        console.log("model loaded")
    }
    loadModel();
    let utils = new Utils('errorMessage');
    let streaming = false;
    utils.loadOpenCv(() => {
        utils.loadCode('codeSnippet', 'codeEditor');
        let eyeCascadeFile = 'haarcascade_eye.xml';
        utils.createFileFromUrl(eyeCascadeFile, "https://grafias-dev.s3.eu-central-1.amazonaws.com/js/haarcascade_eye.xml", () => {})
        let faceCascadeFile = 'haarcascade_frontalface_default.xml';
        utils.createFileFromUrl(faceCascadeFile, "https://grafias-dev.s3.eu-central-1.amazonaws.com/js/haarcascade_frontalface_default.xml", () => {});
        
        let videoInput = document.getElementById('videoInput');
        let startAndStop = document.getElementById('startAndStop');
        let canvasOutput = document.getElementById('canvasOutput');
        let canvasContext = canvasOutput.getContext('2d');
        
        startAndStop.addEventListener('click', () => {
            if (!streaming) {
                utils.clearError();
                utils.startCamera('qvga', onVideoStarted, 'videoInput');
            } else {
                utils.stopCamera();
                onVideoStopped();
            }
        });
        function onVideoStarted() {
            streaming = true;
            startAndStop.innerText = 'Stop';
            videoInput.width = videoInput.videoWidth;
            videoInput.height = videoInput.videoHeight;
            
            utils.executeCode('codeEditor');
        
        }
        
        function onVideoStopped() {
            streaming = false;
            canvasContext.clearRect(0, 0, canvasOutput.width, canvasOutput.height);
            startAndStop.innerText = 'Start';
        }
});

</script>


</body></html>